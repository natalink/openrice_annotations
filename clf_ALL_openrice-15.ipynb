{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 5.178 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import *\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas\n",
    "import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import neighbors\n",
    "num_class = 18\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import time\n",
    "\n",
    "######filtering out non-Chinese comments#########\n",
    "#get_ipython().magic(u'cat comments/uid0.txt | grep -v \"Not comment in Chinese\" > comments/uid0_canto.json') # exclude non-Chinese comments comments\n",
    "#get_ipython().magic(u'cat comments/all_comments | grep -v \"Not comment in Chinese\" > comments/all_canto.json') # exclude non-Chinese comments comments\n",
    "#################################################\n",
    "\n",
    "\n",
    "df_canto = pd.read_json('comments/all_comments_canto.json', lines=True)\n",
    "\n",
    "###########remove reviews without comments\n",
    "df_canto['comment'].replace('', np.nan, inplace=True)\n",
    "df_canto.dropna(subset=['comment'],inplace=True)\n",
    "\n",
    "####training data: X = comment; Y = emoji (1-5 scale)\n",
    "\n",
    "df_15 = df_canto.loc[df_canto['worthy'].isin(['1','5'])]\n",
    "\n",
    "train_x = df_15[['comment']]\n",
    "train_y = df_15[['worthy']]\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "########Tokenizing Cantonese text with jieba########\n",
    "dictionary = list() # segmented text\n",
    "for index, sentence in train_x.iterrows():\n",
    "    comment = sentence['comment']\n",
    "    seg = \" \".join(list(jieba.cut(comment)))\n",
    "    dictionary.append([seg])\n",
    "\n",
    "    \n",
    "####put segmented text into a file#####   \n",
    "with open(\"ALL_15_source_seg_nolabel.txt\", \"w\") as f:\n",
    "    for pair in dictionary:        \n",
    "        f.write(\" \".join(pair))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "documents = TaggedLineDocument(\"ALL_15_source_seg_nolabel.txt\")\n",
    "model = Doc2Vec(documents, vector_size=100, window=8, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "model.save(\"ALL_15_picc_doc2vec.vec\")         \n",
    "end = time.clock()\n",
    "print (\"Total running time: \", end-start)\n",
    "print(\"Most similar to good: \", model.most_similar('好')) # just to test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first review in original format:  comment    雖然貴. 但真心好食嘅爆汁牛肉餅. 九龍城除咗泰國菜出名外. 仲有呢間. 清真牛肉館. 佢地...\n",
      "Name: 0, dtype: object\n",
      "Emoji rank of the first review:  worthy    3\n",
      "Name: 0, dtype: int64\n",
      "first review in segmented format:  ['雖然 貴 .   但 真心 好 食 嘅 爆 汁 牛肉 餅 .   九龍城 除 咗 泰國菜 出名 外 .   仲有 呢 間 .   清真 牛肉 館 .   佢 地 最 出名 嘅 就 係 呢 個 牛肉 餅 .   細細 一件 已經要 .   價錢 確實 有 啲 貴 .   但 味道 真 係 唔 錯 .   外皮 煎 得 好 香脆 .   一咬落 去 .   啲 湯汁 噴晒出 嚟 .   如果 無經驗者 .   真 係 超 容易 整污 糟件 衫 .   其實 我 差少少 都 中招 .   好彩 身手 敏捷 .   避開 咗 .   另外 佢 肉 味 香 濃 .   口感 飽滿 .   以 牛肉 餅呢 講 .   佢 都 算 一 哥 .   如果 佢 係 旺 區 開 分店 就 好 啦 .   咁 就 唔 洗 吓 吓碌入 九龍城 先食 到']\n",
      "first review in doc2vec format:  [-0.03593909 -0.08665214  0.02126475 -0.0515711   0.00361872  0.0712563\n",
      " -0.06323193 -0.03537343  0.04411558  0.1097589   0.06126192  0.04691868\n",
      " -0.0353487  -0.00806015  0.08015794 -0.12773752 -0.02744497  0.03089508\n",
      " -0.00489742  0.08140569 -0.05906105 -0.07337315 -0.01832197 -0.01317464\n",
      "  0.02596863 -0.08374956 -0.02183769 -0.23581041 -0.10425872  0.05293929\n",
      "  0.09718469 -0.0348357   0.03440882  0.03246684  0.00157889  0.00920074\n",
      "  0.06632628  0.04235233  0.03956478  0.07971732 -0.07709208  0.14152583\n",
      " -0.01686089  0.06525242 -0.06560557  0.01603704 -0.11653605  0.02702421\n",
      "  0.08855251 -0.00179176 -0.03943796 -0.04315289  0.09127931 -0.05866153\n",
      " -0.16016474 -0.17276877  0.02549549 -0.12507334  0.11924028 -0.01241123\n",
      " -0.00501597  0.02764781 -0.06620103 -0.07080285 -0.04103263 -0.10394094\n",
      "  0.00507111 -0.11167657  0.00991222  0.01925277 -0.07127433  0.0550409\n",
      "  0.11330946  0.09641464  0.14798264 -0.02798628  0.02418507 -0.01901403\n",
      " -0.02112025  0.03491917  0.15344597  0.05200025 -0.02463372  0.05358073\n",
      "  0.11534563 -0.0234858   0.01534863  0.03439315 -0.02928076  0.03026723\n",
      "  0.10497583 -0.03769602  0.08008529  0.00382708  0.04721628  0.00201947\n",
      "  0.03477791 -0.13160346 -0.1114516   0.02893766]\n"
     ]
    }
   ],
   "source": [
    "#########Testing the model###########\n",
    "\n",
    "print (\"first review in original format: \", train_x.iloc[0] )\n",
    "print (\"Emoji rank of the first review: \", train_y.iloc[0] )\n",
    "print (\"first review in segmented format: \", dictionary[0] )\n",
    "print (\"first review in doc2vec format: \", model.docvecs[0] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_line = len(model.docvecs)\n",
    "print(num_line)\n",
    "\n",
    "#######Put the doc2vec representation for each comment into an array\n",
    "x_vecs = np.zeros((num_line, 100))\n",
    "for i in range(0,num_line):\n",
    "    x_vecs[i] = model.docvecs[i]\n",
    "\n",
    "#######Put y_labels (without column names) into an array    \n",
    "y_label = []\n",
    "for index, label in train_y.iterrows():\n",
    "    label_only = label['worthy']\n",
    "    y_label.append(label_only)\n",
    "\n",
    "######Slice the first 1000 lines to make a \n",
    "######Normally, we should have done some cross-validation\n",
    "train_num = 1000\n",
    "X_test = x_vecs[:train_num]\n",
    "Y_test = y_label[:train_num]\n",
    "X_train = x_vecs[train_num+1:]\n",
    "Y_train = y_label[train_num+1:]\n",
    "\n",
    "########\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_logreg.fit(X_train, Y_train)\n",
    "print (\"Logistic regression: \", clf_logreg.score(X_test, Y_test) )\n",
    "########\n",
    "clf_svc = LinearSVC()\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "print (\"Support Vector Machines: \", clf_svc.score(X_test, Y_test) )\n",
    "########\n",
    "clf_nei = neighbors.KNeighborsClassifier(num_class, weights='distance')\n",
    "clf_nei.fit(X_train, Y_train)\n",
    "print (\"Nearest neighbours: \", clf_nei.score(X_test, Y_test) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "########\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_logreg.fit(X_train, Y_train)\n",
    "print (\"Logistic regression: \", clf_logreg.score(X_test, Y_test) )\n",
    "########\n",
    "clf_svc = LinearSVC()\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "print (\"Support Vector Machines: \", clf_svc.score(X_test, Y_test) )\n",
    "########\n",
    "clf_nei = neighbors.KNeighborsClassifier(num_class, weights='distance')\n",
    "clf_nei.fit(X_train, Y_train)\n",
    "print (\"Nearest neighbours: \", clf_nei.score(X_test, Y_test) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
